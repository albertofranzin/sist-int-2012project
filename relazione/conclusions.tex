\section{Conclusions}
We have seen that the Naive Bayes approach works quite well when classifying a dataset, despite its simplicity. The training requires some hundred of mails, but after this the performance gets better thanks to the bayesian logic, that updates the probability according to its own previous work. Thus, the more mails are processed, the higher accuracy we get. Of course, this holds only if we start from an adequate accuracy: if we use a low quality classifier, our results would remain very bad, as already noted in section \ref{spamthrtest}. When looking at the output, it's also easy to notice that the last mails of the testing set have very few misclassifications (accuracy is a global measure of the validation/test set, so it's affected by the more frequent errors in the first mails), which is a reinforcement for our thesis. Other than accuracy, an almost equal number of false positives and false negatives is also an indication of good tuning.

In the tests described in the previous chapter we have used the same number of spam and ham mails for training, validation and testing, but we have also seen that using a different ratio of the classes does not change the accuracy obtained. This is a very good index of the resilience of Naive Bayes networks as classifiers, so that they can be used ``from scratch'', without any previous knowledge about the data, just starting training, since the network will learn from itself.

Since the network has to be trained on a specific dataset, clearly the results reflect the composition of the mail corpus used. Using the SpamAssassin dataset, we have found that ham mails are likely to contain more links than spam ones; we would have expected the opposite. As expected, the most common words are useless single characters, and common english words like articles, conjunctions, etc. that do not bring any contribution to the mail spamminess.

We have also seen that, with this implementation and this dataset, computing the features brings very little contribution to the status of a mail, and a word-only-based classification is much more accurate.

Note that a more complex system, better if integrated into a mail server or client, which can be trained and tuned using many more and more different emails, can surely achieve higher accuracy, for the intrinsic nature of Bayesian networks. We have already named a few features that can be added to the classifier to improve its performance (see section \ref{featuresused}). Anyway, we have shown that a very simple and ``naive'' Bayesian classifier can work pretty well with relatively little data and a handful of key parameters.

The code can also be surely optimized, with a better knowledge of Python and with some code restructuration.
