\section{Tests and results}
\subsection{Dataset}
The dataset used is the SpamAssassin Public Corpus provided by SpamAssassin at \url{http://spamassassin.apache.org/publiccorpus/}, available for development purposes. It contains about 6400 mails in english language, collected from 2002 to 2005, with approximately the 31\% of the mails being spam.

For testing, we have also used some of spam and ham mails received by us.

\subsection{Modalities}

\subsection{Features of spam and ham mails}
The following features has been selected as relevant and are tracked by the network:
\begin{itemize}[noitemsep]
  \item the number of words written in uppercase characters: since it is considered the equivalent of screaming, many spammers use this gross gimmick to gain attention;
  \item the number of urls found: we expect to find a lot of addresses of websites where to buy goods or where some criminal activity will be performed (phishing, stealing credentials, etc.);
  \item the number of email addresses: for the same reason of the previous feature. Furthermore, spammers operate on a large scale, so it is possible for a spam mail to have multiple recipients;
  \item the number of words shorter than a given threshold (given as a parameter): a trick to fool a system based on word classification is to hide the words. One of the most common and easier way of accomplish this, is to separate each letter with spaces. Since our token separation relies on whitespaces between words, a word like \verb!s p a m!, which a human will easily understand as a single one, will be classified by the network as four different words. In a common, correct plain english text, the ratio of short words like \verb!a! or \verb!I! will be substantially lower, not to mention other random letters;
  \item the number of non-address words longer than a given threshold (parameter): similarly to the previous feature, a spam mail is likely to contain a number of very long, meaningless words much higher than a ham mail. Since there aren't many very long legit words\footnote{The average length of english words is slightly higher than 5 (\url{http://www.puchu.net/doc/Average_Word_Length}), while the majority of words appearing in a text is from 2 to 5 letters long. \citep{STUL:STUL109}}, this is a reasonable feature to track;
  \item the number of words in the form \verb!username@host!: the tests have shown that the ham mails in the SpamAssassin Public Corpus contain many tokens of this type;
  \item the number of words in ``title'' format (first letter capital, remaining letter lowercase);
  \item the number of words in a mail.
\end{itemize}

Many other interesting features can also be used: for example, a heavy use of images as text replacement, the consistency of the email header, the consistency of the urls with the link they claim to go, the list of people the user have mailed before, the provenance of the mail from a known spammer or IP range, the correctness of words (by ckecking a dictionary), to name a few. All of these features require a semantic analysis of the mail, which goes beyond the purpose of this project, since no one of these features is directly connected to Bayesian networks. Nonetheless, all of these features can be used by a Bayesian network, just like the previous ones, and are very likely to increase the accuracy of the classification.

It should be noted that while some of these features are common for the majority of spam, some others are derived from our particular mail archive. Moreover, since many of the mails in the archive are ten years old, more recent spam mails may contain different features. The maintainers of the archive warn to use the archive only for development purposes, not in a live system.

\subsection{The design parameters}
The Bayesian network requires some parameters to be tuned: for example, the ``spamicity'' threshold (how much the normalized probability of being spam should be to be really considered as spam), or the ``weight'' of the feature stats with respect to the word stats when computing the final probability. Different parameter configurations lead to obtain different accuracy values, so we have to choose the best one.

We'll now present the results obtained. Since there are multiple degrees of freedom, we show how the accuracy varies when trying to change one single parameter at time, all the other ones being set to their optimal value.

\paragraph{Size of the training set}
For finding the best size of the training set, we have begun with few mails, then we have tried to increase the size of the training set until the accuracy has started to decrease (the \textit{early stopping} technique).

\begin{center}
\begin{tabular}{ccc}
\toprule
\multicolumn{2}{c}{Size of training set} \\
\cmidrule(r){1-2}
Spam & Ham & Accuracy \\
\midrule
10  & 10    & 0.1 \\
20  & 20    & 0.2 \\
50  & 50    & 0.3 \\
100 & 100   & 0.4 \\
\bottomrule
\end{tabular}
\end{center}

As we see, only 50 spam and 50 ham mails are enough to have best accuracy. Given that we have over six thousand of mails in the archive, it is a very small size.

\paragraph{``Spamicity'' threshold}
Another very important parameter is what is sometimes called the ``spamicity'' threshold, namely the value that the normalized probability $\frac{p_{spam}}{p_{spam} + p_{ham}}$ must reach for the mail to be classified as spam. This is crucial: a low threshold will bring in many false positives (good mails will be marked as spam), while a too high threshold will classify some spam mails as ham, thus having a lot of false negatives. Anyway, the choice of the threshold to be $0.5$ may not be obvious, since we may want to be more tolerant with words we have never met before, or we way consider the chance of false positive too bad for us, and therefore lift the threshold to ensure this fact, allowing at the same time some spam to pass through the filter.

\begin{center}
\begin{tabular}{cccc}
\toprule
Threshold & \shortstack{False\\ positives} & \shortstack{False\\ negatives} & Accuracy\\
\midrule
0.2  & 10 & 0 & 0.1 \\
0.4  & 10 & 0 & 0.1 \\
0.5  & 10 & 0 & 0.1 \\
0.6  & 10 & 0 & 0.1 \\
0.7  & 10 & 0 & 0.1 \\
0.8  & 10 & 0 & 0.1 \\
0.9  & 10 & 0 & 0.1 \\
0.95 & 10 & 0 & 0.1 \\
\bottomrule
\end{tabular}
\end{center}

The highest accuracy is obtaines with the ``trivial'' threshold of $0.5$. However, if we aim to minimize false positives or false negatives, we may prefer other thresholds.

\paragraph{Feature/words stats proportion}
Another problem is to determine how much weight should we assign to the features statistics, and how much to the word statistics.

The final formula for spam is $p_{spam} = W \times p_{fs} + (1-W) \times p_{ws}$, where $p_{spam}$ is the probability of the mail being spam (yet to be normalized), $p_{fs}$ is the probability of being spam computed using the statistics of the features, $p_{ws}$ is the probability of being spam computed using the statistics of the words, and $W$ is the parameter we want to adjust. The formula for ham is the dual.

A value close to $1$ will lead to a classification based solely on the features, thus almost ignoring the actual content of the mail. A low value will instead disregard the features, and therefore many useful and common characteristics of spam mails.

\begin{center}
\begin{tabular}{cc}
\toprule
Threshold & Accuracy \\
\midrule
10    & 0.1 \\
20    & 0.2 \\
50    & 0.3 \\
100   & 0.4 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Analysis of results}
Here we show and analyze some interesting results observed. First of all, here are the feature statistics computed.

\begin{center}
\begin{tabular}{lcc}
\toprule
& \multicolumn{2}{c}{Occurrences} \\
\cmidrule(r){2-3}
Feature & \# in spam & \# in ham \\
\midrule
\# of all-caps words                            & 10    & 0.1 \\
\# of alphanumerical words                      & 20    & 0.2 \\
\# of string in user/hosts form                 & 50    & 0.3 \\
\# of links                                     & 100   & 0.4 \\
\# of mail addresses                            & 10    & 0.1 \\
\# of all lowercase words                       & 20    & 0.2 \\
\# of words with only the first letter capital  & 50    & 0.3 \\
\# of ``short words''                           & 10    & 0.0 \\
\# of non-address ``very long'' words           & 100   & 0.4 \\
\# of non-valid words                           & 50    & 0.3 \\
\# of numbers                                   & 100   & 0.4 \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Length of words}
Clearly, ``very short'' and ``very long'' words (out of our thresholds) are mostly common in spam mails.

\paragraph{Most common words}
It's interesting also to look at the statistics of the single words. We have already discussed about how the distribution of unusually long words. Now, let's take a look at the most common (and untrivial) spam and ham words.
